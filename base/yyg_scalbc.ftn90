!---------------------------------- LICENCE BEGIN -------------------------------
! GEM - Library of kernel routines for the GEM numerical atmospheric model
! Copyright (C) 1990-2010 - Division de Recherche en Prevision Numerique
!                       Environnement Canada
! This library is free software; you can redistribute it and/or modify it
! under the terms of the GNU Lesser General Public License as published by
! the Free Software Foundation, version 2.1 of the License. This library is
! distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
! without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
! PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.
! You should have received a copy of the GNU Lesser General Public License
! along with this library; if not, write to the Free Software Foundation, Inc.,
! 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
!---------------------------------- LICENCE END ---------------------------------
!**s/r yyg_scalbc- to interpolate and exchange scalar data
!
#include "model_macros_f.h"

      Subroutine yyg_scalbc(tab_dst,tab_src,DIST_DIM,NK)
      implicit none
!
!author
!           Abdessamad Qaddouri/V.Lee - October 2009
!
      include 'mpif.h'
#include "ptopo.cdk"
#include "glb_ld.cdk"
#include "geomn.cdk"
#include "geomg.cdk"
#include "glb_pil.cdk"
#include "yyg_pil.cdk"

      integer DIST_DIM,Ni,Nj,NK,numproc
      real  tab_src (DIST_SHAPE,Nk)
      real  tab_dst (DIST_SHAPE,Nk)
      real*8  tab_src_8(DIST_SHAPE,NK)
      integer ierr,i,j,k,kk,kk_proc,m,mm
      integer pil_recvlen(Ptopo_numproc),pil_sendlen(Ptopo_numproc)
      real, dimension (:,:), allocatable :: recv_pil,send_pil
      real sent,recv
      integer status(MPI_STATUS_SIZE)
      integer stat(MPI_STATUS_SIZE,Ptopo_numproc)
      integer request(Ptopo_numproc*2)
      real*8  send_pil_8
      integer tag2,recvlen,sendlen,tag1,ireq
      tag2=14
      tag1=13
      
      sendlen=0
      recvlen=0
      ireq=0
      do kk=1,Ptopo_numproc
         Pil_sendlen(kk)= Pil_sendw_len(kk)+Pil_sende_len(kk) &
                         +Pil_sends_len(kk)+Pil_sendn_len(kk)
         sendlen=max(sendlen,Pil_sendlen(kk))
         Pil_recvlen(kk)= Pil_recvw_len(kk)+Pil_recve_len(kk) &
                         +Pil_recvs_len(kk)+Pil_recvn_len(kk)
         recvlen=max(recvlen,Pil_recvlen(kk))
      enddo
      

!     print *,'sendlen=',sendlen,' recvlen=',recvlen
      if (sendlen.gt.0) then
          allocate(send_pil(sendlen*NK,Ptopo_numproc))
!         assume rpn_comm_xch_halo already done on tab_src
          tab_src_8(:,:,:)=dble(tab_src(:,:,:))
      endif
      if (recvlen.gt.0) then
          allocate(recv_pil(recvlen*NK,Ptopo_numproc))
      endif
 
!
      do 100 kk=1,Ptopo_numproc
!
!        For each processor (in other colour)
         if (Ptopo_couleur.eq.0) then
             kk_proc = kk+Ptopo_numproc-1
         else
             kk_proc = kk-1
         endif

!        prepare to send to other colour processor
         if (Pil_sendlen(kk).gt.0) then
!            prepare something to send

             mm=0

! make for west
             do m=1,Pil_sendw_len(kk)
!$omp parallel private (mm,send_pil_8,i,j) &
!$omp          shared (tab_src_8,send_pil)
!$omp do
                do k=1,NK
!                  mm=mm+1
                   mm=(m-1)*NK+k
                   call int_cub_lag(send_pil_8,tab_src_8(l_minx,l_miny,k), &
                             Pil_sendw_imx(m,kk),Pil_sendw_imy(m,kk),      &
                             LDIST_DIM,                                    &
                             Pil_sendw_xxr(m,kk),Pil_sendw_yyr(m,kk))
                   send_pil(mm,KK)=real(send_pil_8)
                enddo
!$omp enddo
!$omp end parallel
             enddo
! make for east
             do m=1,Pil_sende_len(kk)
!$omp parallel private (mm,send_pil_8,i,j) &
!$omp          shared (tab_src_8,send_pil)
!$omp do
                do k=1,NK
                   mm=(Pil_sendw_len(kk)+m-1)*NK+k
                   call int_cub_lag(send_pil_8,tab_src_8(l_minx,l_miny,k), &
                             Pil_sende_imx(m,kk),Pil_sende_imy(m,kk),      &
                             LDIST_DIM,                                    &
                             Pil_sende_xxr(m,kk),Pil_sende_yyr(m,kk))
                   send_pil(mm,KK)=real(send_pil_8)
                enddo
!$omp enddo
!$omp end parallel
             enddo
! make for south
             do m=1,Pil_sends_len(kk)
!$omp parallel private (mm,send_pil_8,i,j) &
!$omp          shared (tab_src_8,send_pil)
!$omp do
                do k=1,NK
                   mm=(Pil_sendw_len(kk)+Pil_sende_len(kk)+m-1)*NK+k
                   call int_cub_lag(send_pil_8,tab_src_8(l_minx,l_miny,k), &
                             Pil_sends_imx(m,kk),Pil_sends_imy(m,kk),       &
                             LDIST_DIM,                                     &
                             Pil_sends_xxr(m,kk),Pil_sends_yyr(m,kk))
                   send_pil(mm,KK)=real(send_pil_8)
                enddo
!$omp enddo
!$omp end parallel

             enddo
! make for north
             do m=1,Pil_sendn_len(kk)
!$omp parallel private (mm,send_pil_8,i,j) &
!$omp          shared (tab_src_8,send_pil)
!$omp do
                do k=1,NK
                   mm=(Pil_sendw_len(kk)+Pil_sende_len(kk)+Pil_sends_len(kk)+m-1)*NK+k
                   call int_cub_lag(send_pil_8,tab_src_8(l_minx,l_miny,k), &
                             Pil_sendn_imx(m,kk),Pil_sendn_imy(m,kk),      &
                             LDIST_DIM,                                    &
                             Pil_sendn_xxr(m,kk),Pil_sendn_yyr(m,kk))
                   send_pil(mm,KK)=real(send_pil_8)
                enddo
!$omp enddo
!$omp end parallel
             enddo

             ireq = ireq+1
!            print *,'scalbc: sending',Pil_sendlen(kk)*NK,' to ',kk_proc
             call MPI_ISend(send_pil (1,KK),Pil_sendlen(kk)*NK,MPI_REAL, &
                                        kk_proc,tag2+Ptopo_world_myproc, &
                                        MPI_COMM_WORLD,request(ireq),ierr)
         endif
!
!        check to receive from other colour processor
!
         if (Pil_recvlen(kk).gt.0) then
!            detect something to receive

             ireq = ireq+1
!            print *,'scalbc: receiving',Pil_recvlen(kk)*NK,' from ',kk_proc
             call MPI_IRecv(recv_pil(1,KK),Pil_recvlen(kk)*NK,MPI_REAL, &
                    kk_proc,tag2+kk_proc,MPI_COMM_WORLD,request(ireq),ierr)
         endif

 100  continue

! Wait for all done sending and receiving

      call mpi_waitall(ireq,request,stat,ierr)

! Now fill my results if I have received something

      if (recvlen.gt.0) then

          do 200 kk=1,Ptopo_numproc
! fill my west
             mm=0
             do m=1,Pil_recvw_len(kk)
             do k=1,NK
                mm=mm+1
                tab_dst(Pil_recvw_i(m,kk),Pil_recvw_j(m,kk),k)=recv_pil(mm,KK)
             enddo
             enddo
! fill my east
             do m=1,Pil_recve_len(kk)
             do k=1,NK
                mm=mm+1
                tab_dst(Pil_recve_i(m,kk),Pil_recve_j(m,kk),k)=recv_pil(mm,KK)
             enddo
             enddo
! fill my south
             do m=1,Pil_recvs_len(kk)
             do k=1,NK
                mm=mm+1
                tab_dst(Pil_recvs_i(m,kk),Pil_recvs_j(m,kk),k)=recv_pil(mm,KK)
             enddo
             enddo
! fill my north
             do m=1,Pil_recvn_len(kk)
             do k=1,NK
                mm=mm+1
                tab_dst(Pil_recvn_i(m,kk),Pil_recvn_j(m,kk),k)=recv_pil(mm,KK)
             enddo
             enddo

 200  continue

       
      endif
      if (recvlen.gt.0)deallocate(recv_pil)
      if (sendlen.gt.0) deallocate(send_pil)

      return
      end

