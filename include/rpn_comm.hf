#if !defined(_RPN_COMM_MACROS_)
#define _RPN_COMM_MACROS_

#define RPN_COMM_MASTER  0
#define RPN_COMM_DOMAIN  'grid'
#define RPN_COMM_GRID    'grid'
#define RPN_COMM_WORLD   'world'
#define RPN_COMM_INTEGER 'MPI_INTEGER'
#define RPN_COMM_REAL    'MPI_REAL'
#define RPN_COMM_REAL8   'MPI_REAL8'
#define RPN_COMM_LOGICAL 'MPI_LOGICAL'
#define RPN_COMM_CHARACTER 'MPI_CHARACTER'
!TODO: check that RPN_COMM error code follows the <0 patern... use MACRO if provided
#define RPN_COMM_IS_OK(errcode) (errcode >= 0)
#define RPN_COMM_TOPO_X .true.
#define RPN_COMM_TOPO_Y .false.
#define RPN_COMM_TOPO_FILL   .true.
#define RPN_COMM_TOPO_NOFILL .false.

#endif

!TODO: provide an API [interface] for rpn_comm_*

external :: rpn_comm_bcast
external :: rpn_comm_allreduce
external :: rpn_comm_xch_halo
external :: rpn_comm_sendrecv

external :: mpi_allgather  !(sbuf, ns, rtype, rbuf, nr, rtype, comm, ierr)

interface


subroutine rpn_comm_init(Userinit,Pelocal,Petotal,Pex,Pey)
! If not already started, initialize MPI mode and some common blocks 
! (internal use for RPN_COMM routines) and return the topology. 
! If pex and pey are greater than 0, those values will be used and 
! Userinit will not be called.
  external :: Userinit
  integer, intent(out)   :: Pelocal,Petotal
  integer, intent(inout) :: Pex,Pey
! Userinit: User specified subroutine which returns the process topology (subroutine userinit(pex,pey)
! Pelocal : rank of local process
! Petotal : total number of processes
! Pex     : number of PE along x axis: if 0, specified with Userinit
! Pey     : number of PE along y axis: if 0, specified with Userinit
end subroutine rpn_comm_init


integer function rpn_comm_init_multigrid(Userinit,Pelocal,Petotal,Pex,Pey,ngrids)
! If not already started, initialize MPI mode and some common blocks 
! (internal use for RPN_COMM routines) and return the topology
! for a multi grid topology.
! If pex and pey are greater than 0, those values will be used and 
! Userinit will not be called.
  external :: Userinit
  integer, intent(out)   :: Pelocal,Petotal
  integer, intent(inout) :: Pex,Pey
  integer, intent(in)    :: ngrids
! Userinit: User specified subroutine which returns the process topology (subroutine userinit(pex,pey)
! Pelocal : rank of local process
! Petotal : total number of processes
! Pex     : number of PE along x axis: if 0, specified with Userinit
! Pey     : number of PE along y axis: if 0, specified with Userinit
! ngrids  : number of grids
end function rpn_comm_init_multigrid


integer function rpn_comm_mype(me,me_x,me_y)
! Return rank and position of the local process, in regards of his domain
  integer, intent(out) :: me,me_x,me_y
! me  : rank of process in its domain
! me_x: position along X axis in its domain
! me_y: position along Y axis in its domain
end function rpn_comm_mype


integer function rpn_comm_bloc(nblocx,nblocy)
! Creates the needed communicators to use blocks within a domain. 
! Those communicators are named "BLOC" for in-block communication and 
! "BLOCMASTER" for communication between blocks master PE. 
! Blocks are constructed by divinding the number of processes along 
! X by nblocx and the number of processes by nblocy. 
! Note that each block has to be identical, so it it impossible to 
! split 7 processes into 2 blocks (then RPN_COMM_bloc=-1 and 
! an error message appears).
integer, intent(in) :: nblocx,nblocy
! nblocx,nblocy: Number of blocks in X and Y direction
end function rpn_comm_bloc


subroutine rpn_comm_carac(npex,npey,me,medomm,mex,mey,sizex,sizey,ismaster, mymaster, mybloc, myblocx,myblocy,blocme,domname)
! Returns local process info.
  integer, intent(out) :: npex,npey,me,medomm,mex,mey,sizex,sizey,ismaster, mymaster, mybloc, myblocx,myblocy,blocme
  character(len=*),intent(out) :: domname
! npex,npey: Number of PE along x and y
! me       : Rank in the context of communicator "ALL"
! medomm   : Rank in the context of communicator "DOMM"
! mex,mey  : x and y coordinates in domain
! sizex,sizey: Size of blocks along x and y axis
! ismaster : Equals 1 if PE is included in communicator "BLOCMASTER", 0 else
! mymaster : Rank (relative to the domain) of the master of the PE's block
! mybloc   : Rank of PE's block
! myblocx,myblocy: Coordinate of PE's block along x and y
! blocme   : Rank of PE relative to its block
! domname  : Domain name of the current PE
end subroutine rpn_comm_carac


subroutine rpn_comm_rank(domname,rank,ierr)
! Return rank of PE in specified domname/communicator
  character(len=*),intent(in) :: domname
  integer, intent(out) :: rank
  integer, intent(out) :: ierr
! domname : Domain/communicator name
! rank    : rank of process in domname
! ierr    : error code
end subroutine rpn_comm_rank


integer function rpn_comm_topo(nxg,minx,maxx,nxl,nxlmax,halox,nx0,alongx,fill)
! Generate needed information about local tile along a specified axis. 
! The input is the total number of point to divide and the size of the halo. 
! The function will split the domain depending of the topology 
! induced by the PEs.
  integer, intent(in) :: nxg,halox
  logical, intent(in) :: alongx,fill
  integer, intent(out):: minx,maxx,nxl,nxlmax,nx0
! nxg    : number of points of global domain along one axis
! halox  : size of halo
! alongx : logical, true if the domain is divided along x
! fill   : logical, add one more point to maxx if nxlmax is even (for memory optimization purpose)
! minx maxx : dimensions of the local array needed to contain the local tile
! nxl    : number of local points along axis for this PE
! nxlmax : maximum of local points along axis for all PE
! nx0    : Position of the first element of the local PE over nxg
end function rpn_comm_topo


integer function rpn_comm_comm(domname)
! Makes the correspondance between RPN_COMM character communicators and MPI communicators... 
! i.e returns integer MPI communicator corresponding to RPN_COMM character communicator domname
! domname  : Domain name; RPN_COMM character communicator name
  character(len=*),intent(in) :: domname
end function rpn_comm_comm


integer function rpn_comm_datyp(datatype)
! Return the MPI integer datatype code
! corresponding to the RPN_COMM Datatype string
! datatype  :  RPN_COMM type name
  character(len=*),intent(in) :: datatype
end function rpn_comm_datyp


subroutine rpn_comm_barrier(comm, ierr)
! Stub for MPI_barrier routine using the same calling sequence, 
! except for datatypes, operators and communicators. 
! Those are replaced by an equivalent string, described in the table below:
! This RPN_COMM routine is a sync point for all PE's included in 
! the specified communicators. Its primary use is for debugging, 
! it may be useful in some other situations when a task has to be 
! terminated for everyone before starting a new one. 
  character(len=*), intent(in) :: comm
  integer, intent(out) :: ierr
! comm : communicator
! ierr : error code
end subroutine rpn_comm_barrier


subroutine rpn_comm_finalize( ierr)
!This RPN_COMM routine stops the MPI mode. It allows a MPI program to terminate gracefully.
  integer, intent(out) :: ierr
! ierr : error code
end subroutine rpn_comm_finalize


subroutine mpi_comm_split(old_comm, color, key, new_comm, ierr)
   ! Create a new communicator
   integer, intent(in)  :: old_comm, color, key
   integer, intent(out) :: new_comm, ierr
   ! old_comm : 
   ! color    :
   ! key      :
   ! new_comm :
   ! ierr     : error code
end subroutine mpi_comm_split


subroutine mpi_comm_rank(comm, rank, ierr)
   ! Get rank of PE in communicator comm
   integer, intent(in)  :: comm
   integer, intent(out) :: rank, ierr
   ! comm : communicator
   ! rank : rank of process in comm
   ! ierr : error code
end subroutine mpi_comm_rank

end interface
